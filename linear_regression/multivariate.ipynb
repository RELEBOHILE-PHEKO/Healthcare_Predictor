# multivariate.ipynb (Python Notebook Code for Healthcare Cost Prediction)

# --- 1. Imports and Data Load ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDRegressor, LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import joblib

# Load data
df = pd.read_csv("insurance.csv")
df.head()

# --- 2. Data Exploration ---
print(df.info())
print(df.describe())
print(df.isnull().sum())

# Visualizations
sns.pairplot(df, hue="smoker")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# --- 3. Feature Engineering ---
X = df.drop("charges", axis=1)
y = df["charges"]

numeric_features = ["age", "bmi", "children"]
categorical_features = ["sex", "smoker", "region"]

numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(drop="first")

preprocessor = ColumnTransformer([
    ("num", numeric_transformer, numeric_features),
    ("cat", categorical_transformer, categorical_features)
])

# --- 4. Train/Test Split ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 5. Linear Regression with Gradient Descent ---
lin_pipeline = Pipeline([
    ("preprocess", preprocessor),
    ("model", SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.01, learning_rate='constant', random_state=42))
])

lin_pipeline.fit(X_train, y_train)

train_preds = lin_pipeline.predict(X_train)
test_preds = lin_pipeline.predict(X_test)

train_loss = mean_squared_error(y_train, train_preds)
test_loss = mean_squared_error(y_test, test_preds)

print(f"Train MSE: {train_loss:.2f}")
print(f"Test MSE: {test_loss:.2f}")

# --- 6. Scatter Plot ---
plt.scatter(y_test, test_preds, alpha=0.6)
plt.xlabel("Actual Charges")
plt.ylabel("Predicted Charges")
plt.title("Linear Regression: Actual vs Predicted")
plt.plot([0, max(y_test)], [0, max(y_test)], color='red')
plt.grid(True)
plt.show()

# --- 7. Compare Models ---
def evaluate(model):
    pipe = Pipeline([
        ("preprocess", preprocessor),
        ("model", model)
    ])
    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)
    print(model.__class__.__name__)
    print("R2:", r2_score(y_test, preds))
    print("MAE:", mean_absolute_error(y_test, preds))
    print("RMSE:", np.sqrt(mean_squared_error(y_test, preds)))
    print("---\n")
    return pipe, r2_score(y_test, preds)

models = [
    LinearRegression(),
    DecisionTreeRegressor(random_state=42),
    RandomForestRegressor(random_state=42)
]

best_model = None
best_score = -np.inf

for m in models:
    pipe, score = evaluate(m)
    if score > best_score:
        best_score = score
        best_model = pipe

# --- 8. Save Best Model ---
joblib.dump(best_model, "../API/model.pkl")
